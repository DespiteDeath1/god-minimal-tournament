base_model:
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

load_in_8bit: false
load_in_4bit: false
strict: false
bf16: true
fp16: false
tf32: true

adapter: lora
lora_model_dir:
lora_r: 32  # Balanced default
lora_alpha: 64  # 2x lora_r
lora_dropout: 0.05
lora_target_linear: true
lora_target_modules:
  - all-linear

wandb_project: Gradients-On-Demand
wandb_mode: offline
output_dir:

gradient_accumulation_steps: 4  # Good default
micro_batch_size: 4  # Conservative batch size
eval_batch_size: 1
num_epochs: 3
warmup_steps: 100
learning_rate: 0.0002  # Safe default
logging_steps: 1
save_strategy: steps
save_steps: 250

gradient_checkpointing: true  # Memory optimization
fsdp:
fsdp_config:

sample_packing: true
eval_sample_packing: false
pad_to_sequence_len: true

model_kwargs:
  attn_implementation: eager  # Standard attention

dataloader_num_workers: 2
dataloader_pin_memory: false

deepspeed:
weight_decay: 0.0
ddp_timeout: 1800
ddp_bucket_cap_mb: 25

# Safety settings
max_grad_norm: 1.0
seed: 42

# Push to hub settings
hub_strategy: checkpoint
push_to_hub: true

# Evaluation settings
evaluation_strategy: no
val_set_size: 0

# Flash attention disabled for compatibility
flash_attention: false
flash_attn_cross_entropy: false
flash_attn_rms_norm: false

# Dataset processing
dataset_processes: 4
group_by_length: false